2022-11-11 17:43:50,369 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,370 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'glove'
      (embedding): Embedding(400001, 100)
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)
  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=36, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2022-11-11 17:43:50,373 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,374 Corpus: "Corpus: 26 train + 1000 dev + 3850 test sentences"
2022-11-11 17:43:50,374 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,375 Parameters:
2022-11-11 17:43:50,375  - learning_rate: "0.050000"
2022-11-11 17:43:50,376  - mini_batch_size: "32"
2022-11-11 17:43:50,376  - patience: "3"
2022-11-11 17:43:50,377  - anneal_factor: "0.5"
2022-11-11 17:43:50,377  - max_epochs: "10"
2022-11-11 17:43:50,378  - shuffle: "True"
2022-11-11 17:43:50,378  - train_with_dev: "False"
2022-11-11 17:43:50,379  - batch_growth_annealing: "False"
2022-11-11 17:43:50,379 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,379 Model training base path: "resources\taggers\example-ner"
2022-11-11 17:43:50,380 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,380 Device: cpu
2022-11-11 17:43:50,381 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,381 Embeddings storage mode: cpu
2022-11-11 17:43:50,382 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,849 epoch 1 - iter 1/1 - loss 4.40817743 - samples/sec: 68.78 - lr: 0.050000
2022-11-11 17:43:50,850 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:50,851 EPOCH 1 done: loss 4.4082 - lr 0.050000
2022-11-11 17:43:55,504 Evaluating as a multi-label problem: False
2022-11-11 17:43:55,581 DEV : loss 3.999699354171753 - f1-score (micro avg)  0.0027
2022-11-11 17:43:55,659 BAD EPOCHS (no improvement): 0
2022-11-11 17:43:55,659 saving best model
2022-11-11 17:43:56,616 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:57,060 epoch 2 - iter 1/1 - loss 4.00512927 - samples/sec: 72.10 - lr: 0.050000
2022-11-11 17:43:57,062 ----------------------------------------------------------------------------------------------------
2022-11-11 17:43:57,062 EPOCH 2 done: loss 4.0051 - lr 0.050000
2022-11-11 17:44:01,694 Evaluating as a multi-label problem: False
2022-11-11 17:44:01,766 DEV : loss 3.5944535732269287 - f1-score (micro avg)  0.0022
2022-11-11 17:44:01,842 BAD EPOCHS (no improvement): 1
2022-11-11 17:44:01,843 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:02,296 epoch 3 - iter 1/1 - loss 3.56973402 - samples/sec: 70.76 - lr: 0.050000
2022-11-11 17:44:02,297 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:02,297 EPOCH 3 done: loss 3.5697 - lr 0.050000
2022-11-11 17:44:06,952 Evaluating as a multi-label problem: False
2022-11-11 17:44:07,034 DEV : loss 3.159742593765259 - f1-score (micro avg)  0.0025
2022-11-11 17:44:07,109 BAD EPOCHS (no improvement): 2
2022-11-11 17:44:07,110 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:07,562 epoch 4 - iter 1/1 - loss 3.16141242 - samples/sec: 70.99 - lr: 0.050000
2022-11-11 17:44:07,563 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:07,563 EPOCH 4 done: loss 3.1614 - lr 0.050000
2022-11-11 17:44:12,431 Evaluating as a multi-label problem: False
2022-11-11 17:44:12,510 DEV : loss 2.6782941818237305 - f1-score (micro avg)  0.0022
2022-11-11 17:44:12,587 BAD EPOCHS (no improvement): 3
2022-11-11 17:44:12,588 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:13,056 epoch 5 - iter 1/1 - loss 2.64240697 - samples/sec: 68.55 - lr: 0.050000
2022-11-11 17:44:13,057 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:13,058 EPOCH 5 done: loss 2.6424 - lr 0.050000
2022-11-11 17:44:17,709 Evaluating as a multi-label problem: False
2022-11-11 17:44:17,783 DEV : loss 2.148904323577881 - f1-score (micro avg)  0.0029
2022-11-11 17:44:17,878 BAD EPOCHS (no improvement): 0
2022-11-11 17:44:17,879 saving best model
2022-11-11 17:44:18,938 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:19,490 epoch 6 - iter 1/1 - loss 2.05642540 - samples/sec: 58.02 - lr: 0.050000
2022-11-11 17:44:19,491 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:19,491 EPOCH 6 done: loss 2.0564 - lr 0.050000
2022-11-11 17:44:24,303 Evaluating as a multi-label problem: False
2022-11-11 17:44:24,349 DEV : loss 1.5968952178955078 - f1-score (micro avg)  0.0055
2022-11-11 17:44:24,447 BAD EPOCHS (no improvement): 0
2022-11-11 17:44:24,449 saving best model
2022-11-11 17:44:25,524 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:26,044 epoch 7 - iter 1/1 - loss 1.58668920 - samples/sec: 61.70 - lr: 0.050000
2022-11-11 17:44:26,045 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:26,045 EPOCH 7 done: loss 1.5867 - lr 0.050000
2022-11-11 17:44:30,168 Evaluating as a multi-label problem: False
2022-11-11 17:44:30,196 DEV : loss 1.1010242700576782 - f1-score (micro avg)  0.003
2022-11-11 17:44:30,278 BAD EPOCHS (no improvement): 1
2022-11-11 17:44:30,280 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:30,763 epoch 8 - iter 1/1 - loss 1.05490414 - samples/sec: 66.50 - lr: 0.050000
2022-11-11 17:44:30,763 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:30,764 EPOCH 8 done: loss 1.0549 - lr 0.050000
2022-11-11 17:44:35,066 Evaluating as a multi-label problem: False
2022-11-11 17:44:35,085 DEV : loss 0.7665504813194275 - f1-score (micro avg)  0.0
2022-11-11 17:44:35,167 BAD EPOCHS (no improvement): 2
2022-11-11 17:44:35,168 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:35,632 epoch 9 - iter 1/1 - loss 0.72658666 - samples/sec: 69.15 - lr: 0.050000
2022-11-11 17:44:35,633 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:35,634 EPOCH 9 done: loss 0.7266 - lr 0.050000
2022-11-11 17:44:39,810 Evaluating as a multi-label problem: False
2022-11-11 17:44:39,829 DEV : loss 0.610571563243866 - f1-score (micro avg)  0.0
2022-11-11 17:44:39,920 BAD EPOCHS (no improvement): 3
2022-11-11 17:44:39,921 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:40,382 epoch 10 - iter 1/1 - loss 0.60761770 - samples/sec: 69.45 - lr: 0.050000
2022-11-11 17:44:40,383 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:40,384 EPOCH 10 done: loss 0.6076 - lr 0.050000
2022-11-11 17:44:44,631 Evaluating as a multi-label problem: False
2022-11-11 17:44:44,651 DEV : loss 0.5669580101966858 - f1-score (micro avg)  0.0
2022-11-11 17:44:44,736 Epoch    10: reducing learning rate of group 0 to 2.5000e-02.
2022-11-11 17:44:44,737 BAD EPOCHS (no improvement): 4
2022-11-11 17:44:45,791 ----------------------------------------------------------------------------------------------------
2022-11-11 17:44:45,793 loading file resources\taggers\example-ner\best-model.pt
2022-11-11 17:44:46,446 SequenceTagger predicts: Dictionary with 36 tags: <unk>, O, S-facility, B-facility, E-facility, I-facility, S-geo-loc, B-geo-loc, E-geo-loc, I-geo-loc, S-other, B-other, E-other, I-other, S-company, B-company, E-company, I-company, S-product, B-product, E-product, I-product, S-person, B-person, E-person, I-person, S-sportsteam, B-sportsteam, E-sportsteam, I-sportsteam, S-movie, B-movie, E-movie, I-movie, <START>, <STOP>
2022-11-11 17:45:02,961 Evaluating as a multi-label problem: False
2022-11-11 17:45:03,090 0.0018	0.0046	0.0026	0.0013
2022-11-11 17:45:03,091 
Results:
- F-score (micro) 0.0026
- F-score (macro) 0.0009
- Accuracy 0.0013

By class:
              precision    recall  f1-score   support

     product     0.0010    0.0203    0.0020       246
  sportsteam     0.0037    0.0748    0.0070       147
      person     0.0000    0.0000    0.0000       482
     geo-loc     0.0000    0.0000    0.0000       882
     company     0.0000    0.0000    0.0000       621
       other     0.0000    0.0000    0.0000       584
    facility     0.0000    0.0000    0.0000       253
 musicartist     0.0000    0.0000    0.0000       191
       movie     0.0000    0.0000    0.0000        34
      tvshow     0.0000    0.0000    0.0000        33

   micro avg     0.0018    0.0046    0.0026      3473
   macro avg     0.0005    0.0095    0.0009      3473
weighted avg     0.0002    0.0046    0.0004      3473

2022-11-11 17:45:03,095 ----------------------------------------------------------------------------------------------------
